{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Handwritten Digit Clasification using Deep Learning (Neural Network).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOi2K6jBSJb/8YuhtacZDWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nigamharshit/MNIST_Handwritten_Digit_Clasification_using_Deep_Learning_-Neural_Network-/blob/main/MNIST_Handwritten_Digit_Clasification_using_Deep_Learning_(Neural_Network).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6KSmO3OM2c2I",
        "outputId": "4fefebc1-79eb-4bd7-db48-da0b66239268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of X_train is:  <class 'numpy.ndarray'>\n",
            "Type of Y_train is:  <class 'numpy.ndarray'>\n",
            "\n",
            "Shape of X_train is:  (60000, 28, 28)\n",
            "Shape of Y_train is:  (60000,)\n",
            "Shape of X_test is:  (10000, 28, 28)\n",
            "Shape of Y_tset is:  (10000,)\n",
            "\n",
            "Numerical Data 10th Image from X_train:\n",
            ":  [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  42 118 219 166 118 118   6\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 103 242 254 254 254 254 254  66\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  18 232 254 254 254 254 254 238\n",
            "   70   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 104 244 254 224 254 254 254\n",
            "  141   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0 207 254 210 254 254 254\n",
            "   34   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  84 206 254 254 254 254\n",
            "   41   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  24 209 254 254 254\n",
            "  171   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  91 137 253 254 254 254\n",
            "  112   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  40 214 250 254 254 254 254 254\n",
            "   34   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  81 247 254 254 254 254 254 254\n",
            "  146   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 110 246 254 254 254 254 254\n",
            "  171   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  73  89  89  93 240 254\n",
            "  171   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 128 254\n",
            "  219  31   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7 254 254\n",
            "  214  28   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 138 254 254\n",
            "  116   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  19 177  90   0   0   0   0   0  25 240 254 254\n",
            "   34   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 164 254 215  63  36   0  51  89 206 254 254 139\n",
            "    8   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  57 197 254 254 222 180 241 254 254 253 213  11\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 140 105 254 254 254 254 254 254 236   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   7 117 117 165 254 254 239  50   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "Shape of Numerical Data 10th Image from X_train:  (28, 28)\n",
            "\n",
            "Displaying the of 10th Image from X_train:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Print the corresponding label of 10th Image from Y_train:  3\n",
            "\n",
            "Unique values in Y_train:  [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique values in Y_test:  [0 1 2 3 4 5 6 7 8 9]\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 1.6368 - accuracy: 0.8169\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3671 - accuracy: 0.9085\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3053 - accuracy: 0.9214\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2607 - accuracy: 0.9311\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2253 - accuracy: 0.9398\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1992 - accuracy: 0.9452\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1770 - accuracy: 0.9507\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1568 - accuracy: 0.9556\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1471 - accuracy: 0.9580\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1373 - accuracy: 0.9603\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 19.1481 - accuracy: 0.9725\n",
            "Accuracy of Test data:  0.9725000262260437\n",
            "\n",
            "Confusion Matrix:\n",
            "  tf.Tensor(\n",
            "[[ 978    0    0    0    0    1    0    0    1    0]\n",
            " [   3 1130    0    0    0    2    0    0    0    0]\n",
            " [  97  376  558    0    0    0    1    0    0    0]\n",
            " [   6  347   74  583    0    0    0    0    0    0]\n",
            " [  32   13   81    5  851    0    0    0    0    0]\n",
            " [  16  297    4  231    8  336    0    0    0    0]\n",
            " [ 566   14   59    5   51  196   67    0    0    0]\n",
            " [  28  439  424   32   47    5    1   52    0    0]\n",
            " [  46  143  405  165   20   93   22    4   76    0]\n",
            " [  37   65   32  271  466   22    1   39   29   47]], shape=(10, 10), dtype=int32)\n",
            "\n",
            "Path of the image to be predicted: /content/download (2).png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=142x141 at 0x7F748F1FA450>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Handwritten Digit is recognised as  7\n"
          ]
        }
      ],
      "source": [
        "# Importing the Dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(3)\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.math import confusion_matrix\n",
        "import pickle\n",
        "\n",
        "# Importing the Datasets\n",
        "(X_train, Y_train), (X_test, Y_test) =  mnist.load_data()\n",
        "\n",
        "def eda():\n",
        "  # Explorartory Data Analysis\n",
        "\n",
        "  print(\"Type of X_train is: \",type(X_train))\n",
        "  print(\"Type of Y_train is: \",type(Y_train))\n",
        "  print()\n",
        "  print(\"Shape of X_train is: \",X_train.shape)\n",
        "  print(\"Shape of Y_train is: \",Y_train.shape)\n",
        "  print(\"Shape of X_test is: \",X_test.shape)\n",
        "  print(\"Shape of Y_tset is: \",Y_test.shape)\n",
        "  print()\n",
        "  print(\"Numerical Data 10th Image from X_train:\\n: \",X_train[10])\n",
        "  print(\"Shape of Numerical Data 10th Image from X_train: \",X_train[10].shape)\n",
        "  print()\n",
        "  print(\"Displaying the of 10th Image from X_train:\\n\")\n",
        "  plt.imshow(X_train[10])\n",
        "  plt.show()\n",
        "  print(\"Print the corresponding label of 10th Image from Y_train: \",Y_train[10])\n",
        "  print()\n",
        "  print(\"Unique values in Y_train: \",np.unique(Y_train))\n",
        "  print(\"Unique values in Y_test: \",np.unique(Y_test))\n",
        "\n",
        "def data_preprocessing(X_train,X_test):\n",
        "  # Data Preprocessing\n",
        "\n",
        "  # Re-scaling the Images\n",
        "  X_train = X_train/255\n",
        "  X_test = X_test/255\n",
        "\n",
        "  # All the Images are already in Dimension 28*28, so no need to do resizng\n",
        "\n",
        "  # all the Images are already in Grayscale, so no need to do Grayscale Conversion\n",
        "\n",
        "def model_train():\n",
        "\n",
        "  # Setting up the layers of the Neural  Network\n",
        "  model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape=(28,28)),\n",
        "                          keras.layers.Dense(50, activation='relu'),\n",
        "                          keras.layers.Dense(50, activation='relu'),\n",
        "                          keras.layers.Dense(10, activation='sigmoid')])\n",
        "\n",
        "  # Compiling the Neural Network\n",
        "  model.compile(optimizer='adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  # Training the Neural Network\n",
        "  model.fit(X_train, Y_train, epochs=10)\n",
        "\n",
        "def test_results():\n",
        "  # Accuracy on Test Data\n",
        "  loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "  print(\"Accuracy of Test data: \",accuracy)\n",
        "  print()\n",
        "\n",
        "  # model.predict() gives the prediction probability of each class for that data point\n",
        "\n",
        "  # Predicting the Test Data\n",
        "  Y_pred = model.predict(X_test)\n",
        "\n",
        "  # Converting the prediction probabilities to class label for all test data points\n",
        "  # It will select the maximum probablity to which class it belongs\n",
        "  Y_pred_labels = [np.argmax(i) for i in Y_pred]\n",
        "\n",
        "  # Print Confusion Matrix\n",
        "  conf_mat = confusion_matrix(Y_test, Y_pred_labels)\n",
        "  print(\"Confusion Matrix:\\n \",conf_mat)\n",
        "  print()\n",
        "\n",
        "def predictive_system(model):\n",
        "  # Building a Predictive System\n",
        "\n",
        "  input_image_path = input('Path of the image to be predicted: ')\n",
        "\n",
        "  input_image = cv2.imread(input_image_path)\n",
        "\n",
        "  cv2_imshow(input_image)\n",
        "\n",
        "  grayscale = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "  input_image_resize = cv2.resize(grayscale, (28, 28))\n",
        "\n",
        "  input_image_resize = input_image_resize/255\n",
        "\n",
        "  image_reshaped = np.reshape(input_image_resize, [1,28,28])\n",
        "\n",
        "  input_prediction = model.predict(image_reshaped)\n",
        "\n",
        "  input_pred_label = np.argmax(input_prediction)\n",
        "\n",
        "  print('The Handwritten Digit is recognised as ', input_pred_label)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # For Exploratory Data Analysis\n",
        "  eda()\n",
        "\n",
        "  # For Data Preprocessing\n",
        "  data_preprocessing(X_train,X_test)\n",
        "\n",
        "  # For training the model\n",
        "  model_train()\n",
        "\n",
        "  # For model Accuracy\n",
        "  test_results()\n",
        "\n",
        "  # For predicting a single image\n",
        "  predictive_system(model)\n"
      ]
    }
  ]
}